import os
import json
import pandas as pd
import requests
import math
import io
import time
from datetime import datetime, timedelta, date
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
from dateutil import parser

# ================= [ë¹„ë°€ë²ˆí˜¸ ë¡œë“œ] =================
try:
    from secrets import MY_KAKAO_KEY, MY_FOLDER_ID, MY_NOTION_KEY, MY_NOTION_DB_ID
    GDRIVE_SA_KEY = None 
    print("ðŸ’» ë‚´ ì»´í“¨í„° ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
except ImportError:
    MY_KAKAO_KEY = os.environ.get("KAKAO_API_KEY")
    MY_FOLDER_ID = os.environ.get("GDRIVE_FOLDER_ID")
    MY_NOTION_KEY = os.environ.get("NOTION_KEY")
    MY_NOTION_DB_ID = os.environ.get("NOTION_DB_ID")
    GDRIVE_SA_KEY = os.environ.get("GDRIVE_SA_KEY")
    print("â˜ï¸ Github ì„œë²„ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")

# ================= [ì„¤ì •ê°’] =================
# [ìžë™ ëª¨ë“œ ì „í™˜ ê¸°ì¤€ì¼]
AUTO_SWITCH_DATE = date(2026, 1, 10) 

IS_CSV_UTC = False  
SMOOTHING_WINDOW = 3
ACCURACY_LIMIT = 50
STAY_RADIUS = 100
MIN_STAY_MINUTES = 5
MERGE_TIME_GAP_MINUTES = 30

MY_TAG_RULES = {
    "ë§ˆíŠ¸": "ðŸ›’ Market", "íŽ¸ì˜ì ": "ðŸ›’ Market", "í•™êµ": "ðŸ« School", "ì´ˆë“±": "ðŸ« School",
    "ì—­": "ðŸš† Station", "ì¹´íŽ˜": "â˜• Cafe", "ì»¤í”¼": "â˜• Cafe", "ë‹¤ì´ì†Œ": "ðŸ›ï¸ Shopping",
    "ì§‘": "ðŸ  Home", "ì•„íŒŒíŠ¸": "ðŸ  Home"
}

# ================= [ê¸°ëŠ¥ í•¨ìˆ˜] =================
def get_credentials():
    if os.path.exists('service_account.json'):
        return service_account.Credentials.from_service_account_file('service_account.json', scopes=['https://www.googleapis.com/auth/drive.readonly'])
    elif GDRIVE_SA_KEY:
        info = json.loads(GDRIVE_SA_KEY)
        return service_account.Credentials.from_service_account_info(info, scopes=['https://www.googleapis.com/auth/drive.readonly'])
    return None

def get_kakao_key(): return MY_KAKAO_KEY
def get_folder_id(): return MY_FOLDER_ID
def get_notion_key(): return MY_NOTION_KEY
def get_notion_db_id(): return MY_NOTION_DB_ID

def format_duration(minutes):
    minutes = int(minutes)
    hours = minutes // 60
    mins = minutes % 60
    return f"{hours}ì‹œê°„ {mins}ë¶„" if hours > 0 else f"{mins}ë¶„"

def haversine(lat1, lon1, lat2, lon2):
    R = 6371000
    phi1, phi2 = math.radians(lat1), math.radians(lat2)
    dphi = math.radians(lat2 - lat1)
    dlambda = math.radians(lon2 - lon1)
    a = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2) * math.sin(dlambda/2)**2
    return 2 * R * math.atan2(math.sqrt(a), math.sqrt(1-a))

def search_place_by_name(keyword, lat, lng):
    api_key = get_kakao_key()
    headers = {"Authorization": f"KakaoAK {api_key}"}
    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    try:
        params = {"query": keyword}
        if lat and lng:
            params.update({"x": lng, "y": lat, "radius": 1000, "sort": "distance"})
        resp = requests.get(url, headers=headers, params=params, timeout=3)
        if resp.status_code == 200:
            data = resp.json()
            if data['meta']['total_count'] > 0:
                doc = data['documents'][0]
                return doc['place_name'], (doc['road_address_name'] or doc['address_name'])
    except: pass
    return keyword, None

def sync_fix_and_learn():
    print("ðŸ”„ ë…¸ì…˜ ë°ì´í„° ì½ì–´ì˜¤ëŠ” ì¤‘...")
    url = f"https://api.notion.com/v1/databases/{get_notion_db_id()}/query"
    headers = {"Authorization": f"Bearer {get_notion_key()}", "Content-Type": "application/json", "Notion-Version": "2022-06-28"}
    
    # ê³¼ê±° ë°ì´í„°ë¥¼ ë§Žì´ ë„£ì„ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ ìµœê·¼ 200ê°œê¹Œì§€ í™•ì¸
    payload = {"page_size": 200, "sorts": [{"property": "ë°©ë¬¸ì¼ì‹œ", "direction": "descending"}]}
    
    existing_ranges = [] 
    name_tag_memory = {} 
    
    try:
        resp = requests.post(url, headers=headers, json=payload)
        if resp.status_code == 200:
            results = resp.json().get("results", [])
            print(f"ðŸ“Š ë…¸ì…˜ ê¸°ì¡´ ê¸°ë¡ {len(results)}ê°œ ë¡œë“œ ì™„ë£Œ.")
            
            for page in results:
                props = page.get("properties", {})
                date_prop = props.get("ë°©ë¬¸ì¼ì‹œ", {}).get("date", {})
                if date_prop:
                    start_str = date_prop.get("start")
                    end_str = date_prop.get("end")
                    if start_str and end_str:
                        try:
                            s_dt = parser.parse(start_str).replace(tzinfo=None)
                            e_dt = parser.parse(end_str).replace(tzinfo=None)
                            title_prop = props.get("ì´ë¦„", {}).get("title", [])
                            p_name = title_prop[0].get("text", {}).get("content", "") if title_prop else "Unknown"
                            existing_ranges.append((s_dt, e_dt, p_name))
                        except: pass
                
                title_prop = props.get("ì´ë¦„", {}).get("title", [])
                p_name = title_prop[0].get("text", {}).get("content", "") if title_prop else ""
                tag_prop = props.get("íƒœê·¸", {}).get("multi_select", [])
                if p_name and tag_prop:
                    name_tag_memory[p_name] = tag_prop[0]['name']

    except Exception as e:
        print(f"âš ï¸ ë…¸ì…˜ ì½ê¸° ì—ëŸ¬: {e}")
        
    return existing_ranges, name_tag_memory

def get_geo_info(lat, lng):
    api_key = get_kakao_key()
    headers = {"Authorization": f"KakaoAK {api_key}"}
    url_addr = "https://dapi.kakao.com/v2/local/geo/coord2address.json"
    address_str = "ì£¼ì†Œ ë¯¸í™•ì¸"; place_name = ""
    try:
        resp = requests.get(url_addr, headers=headers, params={"x": lng, "y": lat}, timeout=3)
        if resp.status_code == 200:
            data = resp.json()
            if data['meta']['total_count'] > 0:
                doc = data['documents'][0]
                if doc['road_address']:
                    address_str = doc['road_address']['address_name']
                    if doc['road_address']['building_name']: place_name = doc['road_address']['building_name']
                else: address_str = doc['address']['address_name']
    except: pass

    if address_str != "ì£¼ì†Œ ë¯¸í™•ì¸":
        url_kwd = "https://dapi.kakao.com/v2/local/search/keyword.json"
        try:
            params = {"query": address_str, "x": lng, "y": lat, "radius": 50, "sort": "distance"}
            resp = requests.get(url_kwd, headers=headers, params=params, timeout=3)
            if resp.status_code == 200:
                data = resp.json()
                if data['meta']['total_count'] > 0: place_name = data['documents'][0]['place_name']
        except: pass
    if not place_name: place_name = address_str
    return place_name, address_str

def is_overlapping(new_start, new_end, existing_ranges):
    ns = new_start.replace(tzinfo=None)
    ne = new_end.replace(tzinfo=None)
    for (ex_start, ex_end, ex_name) in existing_ranges:
        time_diff = abs((ns - ex_start).total_seconds())
        if time_diff < 120: 
            return True, ex_name
    return False, None

def send_to_notion(visit_data, existing_ranges, name_tag_memory):
    is_dup, dup_name = is_overlapping(visit_data['start'], visit_data['end'], existing_ranges)
    
    if is_dup:
        print(f"ðŸ›¡ï¸ [ì¤‘ë³µ ì°¨ë‹¨] íŒ¨ìŠ¤: {dup_name} ({visit_data['start'].strftime('%m/%d %H:%M')})")
        return

    final_tag = "ðŸ“ ê¸°íƒ€"
    if visit_data['place_name'] in name_tag_memory:
        final_tag = name_tag_memory[visit_data['place_name']]
    else:
        for k, t in MY_TAG_RULES.items():
            if k in visit_data['place_name']: final_tag = t; break

    url = "https://api.notion.com/v1/pages"
    headers = {"Authorization": f"Bearer {get_notion_key()}", "Content-Type": "application/json", "Notion-Version": "2022-06-28"}
    
    payload = {
        "parent": {"database_id": get_notion_db_id()},
        "properties": {
            "ì´ë¦„": {"title": [{"text": {"content": visit_data['place_name']}}]},
            "ì£¼ì†Œ": {"rich_text": [{"text": {"content": visit_data['address']}}]},
            "íƒœê·¸": {"multi_select": [{"name": final_tag}]},
            "ì²´ë¥˜ì‹œê°„": {"rich_text": [{"text": {"content": format_duration(visit_data['duration'])}}]},
            "ë°©ë¬¸ì¼ì‹œ": {"date": {"start": visit_data['start'].isoformat(), "end": visit_data['end'].isoformat()}},
            "Lat": {"number": visit_data['lat']},
            "Lon": {"number": visit_data['lon']}
        }
    }

    try:
        resp = requests.post(url, headers=headers, json=payload)
        if resp.status_code == 200:
            print(f"âœ… ë“±ë¡: {visit_data['place_name']} ({visit_data['start'].strftime('%m/%d %H:%M')})")
            existing_ranges.append((visit_data['start'].replace(tzinfo=None), visit_data['end'].replace(tzinfo=None), visit_data['place_name']))
        else: print(f"âŒ ì‹¤íŒ¨: {resp.text}")
    except Exception as e: print(f"âŒ ì—ëŸ¬: {e}")

# [í•µì‹¬] ë‚ ì§œì— ë”°ë¼ íŒŒì¼ ê°€ì ¸ì˜¤ëŠ” ë°©ì‹ì„ ë°”ê¿ˆ
def download_target_files():
    creds = get_credentials()
    if not creds: return []
    service = build('drive', 'v3', credentials=creds)
    folder_id = get_folder_id()
    
    today = datetime.now().date()
    
    # 1. ë‚ ì§œ í™•ì¸ ë° ëª¨ë“œ ê²°ì •
    if today < AUTO_SWITCH_DATE:
        print(f"ðŸ—“ï¸ ì˜¤ëŠ˜ì€ {today}ìž…ë‹ˆë‹¤. (ê¸°ì¤€ì¼ {AUTO_SWITCH_DATE} ì´ì „)")
        print("ðŸ“‚ [ì „ì²´ ëª¨ë“œ] ê³¼ê±° ë°ì´í„°ë¥¼ í¬í•¨í•´ ëª¨ë“  CSV íŒŒì¼ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.")
        # ê³¼ê±° íŒŒì¼ë¶€í„° ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ createdTime asc(ì˜¤ë¦„ì°¨ìˆœ) ì‚¬ìš©
        query_params = {'orderBy': 'createdTime asc', 'pageSize': 100} 
    else:
        print(f"ðŸ—“ï¸ ì˜¤ëŠ˜ì€ {today}ìž…ë‹ˆë‹¤. (ê¸°ì¤€ì¼ {AUTO_SWITCH_DATE} ì´í›„)")
        print("ðŸ“‚ [ìµœì‹  ëª¨ë“œ] ê°€ìž¥ ìµœê·¼ íŒŒì¼ 1ê°œë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.")
        # ìµœì‹  íŒŒì¼ë§Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ createdTime desc(ë‚´ë¦¼ì°¨ìˆœ) ì‚¬ìš©
        query_params = {'orderBy': 'createdTime desc', 'pageSize': 1}

    results = service.files().list(
        q=f"'{folder_id}' in parents and trashed=false",
        fields="files(id, name, mimeType, createdTime)",
        **query_params
    ).execute()
    
    items = results.get('files', [])
    if not items: 
        print("âŒ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return []

    downloaded_files = []
    print(f"ðŸ”Ž ì´ {len(items)}ê°œì˜ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    for item in items:
        if not item['name'].lower().endswith('.csv'): continue
        
        print(f"   â¬‡ï¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {item['name']}")
        fh = io.BytesIO()
        if 'application/vnd.google-apps' in item['mimeType']:
            request = service.files().export_media(fileId=item['id'], mimeType='text/csv')
        else:
            request = service.files().get_media(fileId=item['id'])
            
        downloader = MediaIoBaseDownload(fh, request)
        done = False
        while done is False: status, done = downloader.next_chunk()
        fh.seek(0)
        downloaded_files.append((pd.read_csv(fh), item['name']))
        
    return downloaded_files

def process_clustering(df):
    points = df.to_dict('records')
    if not points: return []
    
    raw_visits = []
    anchor = points[0]
    cluster = [anchor]

    for i in range(1, len(points)):
        curr = points[i]
        dist = haversine(anchor['smooth_lat'], anchor['smooth_lon'], curr['smooth_lat'], curr['smooth_lon'])

        if dist < STAY_RADIUS:
            cluster.append(curr)
        else:
            start_t = cluster[0]['datetime']; end_t = cluster[-1]['datetime']
            duration = (end_t - start_t).total_seconds() / 60

            if duration >= MIN_STAY_MINUTES:
                avg_lat = sum(p['smooth_lat'] for p in cluster) / len(cluster)
                avg_lon = sum(p['smooth_lon'] for p in cluster) / len(cluster)
                api_name, api_addr = get_geo_info(avg_lat, avg_lon)
                
                raw_visits.append({
                    'place_name': api_name, 'address': api_addr, 'lat': avg_lat, 'lon': avg_lon,
                    'start': start_t, 'end': end_t, 'duration': duration
                })
            anchor = curr; cluster = [curr]

    if cluster:
        start_t = cluster[0]['datetime']; end_t = cluster[-1]['datetime']
        duration = (end_t - start_t).total_seconds() / 60
        if duration >= MIN_STAY_MINUTES:
            avg_lat = sum(p['smooth_lat'] for p in cluster) / len(cluster)
            avg_lon = sum(p['smooth_lon'] for p in cluster) / len(cluster)
            api_name, api_addr = get_geo_info(avg_lat, avg_lon)
            raw_visits.append({
                'place_name': api_name, 'address': api_addr, 'lat': avg_lat, 'lon': avg_lon,
                'start': start_t, 'end': end_t, 'duration': duration
            })
            
    return raw_visits

def merge_consecutive_visits(visits):
    if not visits: return []
    merged = [visits[0]]
    
    for current in visits[1:]:
        last = merged[-1]
        is_same_place = (current['place_name'] == last['place_name']) or \
                        (current['address'].replace(" ", "") == last['address'].replace(" ", ""))
        time_gap = (current['start'] - last['end']).total_seconds() / 60
        
        if is_same_place and time_gap <= MERGE_TIME_GAP_MINUTES:
            last['end'] = current['end'] 
            last['duration'] = (last['end'] - last['start']).total_seconds() / 60
        else:
            merged.append(current)
            
    return merged

def main():
    print("ðŸš€ [GPS ë¶„ì„ê¸°] v2.1 (ìŠ¤ë§ˆíŠ¸ ë‚ ì§œ ëª¨ë“œ)")
    
    existing_ranges, name_tag_memory = sync_fix_and_learn()
    
    # ì—¬ê¸°ì„œ ë‚ ì§œì— ë”°ë¼ íŒŒì¼ 1ê°œ ë˜ëŠ” ì—¬ëŸ¬ ê°œë¥¼ ë°›ì•„ì˜µë‹ˆë‹¤
    file_list = download_target_files()
    
    if not file_list: return

    # íŒŒì¼ì´ ì—¬ëŸ¬ ê°œì¼ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ ë°˜ë³µë¬¸ìœ¼ë¡œ ì²˜ë¦¬
    for df, filename in file_list:
        print(f"\nðŸ“„ [íŒŒì¼ ì²˜ë¦¬ ì‹œìž‘] {filename}")
        
        df.columns = df.columns.str.strip().str.lower()
        if 'time' not in df.columns and 'date' in df.columns: 
            df['time'] = df['date'] + ' ' + df['time']
        df['datetime'] = pd.to_datetime(df['time'])

        if IS_CSV_UTC:
            df['datetime'] = df['datetime'] + timedelta(hours=9)
        
        df = df.sort_values('datetime')
        if 'accuracy' in df.columns: df = df[df['accuracy'] <= ACCURACY_LIMIT]
        
        if len(df) == 0: 
            print("   âš ï¸ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            continue

        df['smooth_lat'] = df['lat'].rolling(window=SMOOTHING_WINDOW, min_periods=1, center=True).mean()
        df['smooth_lon'] = df['lon'].rolling(window=SMOOTHING_WINDOW, min_periods=1, center=True).mean()

        raw_visits = process_clustering(df)
        final_visits = merge_consecutive_visits(raw_visits)

        print(f"   ðŸ‘‰ ë°©ë¬¸ ê¸°ë¡ {len(final_visits)}ê±´ ë°œê²¬. ë…¸ì…˜ ì „ì†¡ ì‹œìž‘...")
        for visit in final_visits:
            # ì¤‘ë³µ ì²´í¬í•˜ë©´ì„œ ì „ì†¡ (ì´ë¯¸ ë“±ë¡ë˜ë©´ existing_rangesì— ì¶”ê°€ë˜ì–´ ë‹¤ìŒ íŒŒì¼ ì²˜ë¦¬ ë•Œë„ ë°©ì–´ë¨)
            send_to_notion(visit, existing_ranges, name_tag_memory)

    print(f"\nðŸŽ‰ ëª¨ë“  ìž‘ì—… ì™„ë£Œ!")

if __name__ == "__main__":
    main()
