import os
import json
import math
import io
import pandas as pd
from datetime import datetime
import requests
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload

# ================== ì‚¬ìš©ì ì„¤ì • ==================
GDRIVE_FOLDER_ID = "10tC3MvA9gzjv1E3rdBDi6ZyMaf4lSEni"
NOTION_KEY = os.environ.get("NOTION_KEY")
NOTION_DB_ID = "2ddb9d7d1d4a81028e19d09a1386f820"

ACCURACY_LIMIT = 50
MIN_MOVE_DISTANCE = 30  # meters
# ===============================================

def get_credentials():
    if os.path.exists("service_account.json"):
        return service_account.Credentials.from_service_account_file(
            "service_account.json",
            scopes=["https://www.googleapis.com/auth/drive.readonly"],
        )
    info = json.loads(os.environ["GDRIVE_SA_KEY"])
    return service_account.Credentials.from_service_account_info(
        info,
        scopes=["https://www.googleapis.com/auth/drive.readonly"],
    )


def haversine(lat1, lon1, lat2, lon2):
    R = 6371000
    phi1, phi2 = math.radians(lat1), math.radians(lat2)
    dphi = math.radians(lat2 - lat1)
    dl = math.radians(lon2 - lon1)
    a = math.sin(dphi / 2) ** 2 + math.cos(phi1) * math.cos(phi2) * math.sin(dl / 2) ** 2
    return 2 * R * math.atan2(math.sqrt(a), math.sqrt(1 - a))


def download_all_csv():
    creds = get_credentials()
    service = build("drive", "v3", credentials=creds)

    results = service.files().list(
        q=f"'{GDRIVE_FOLDER_ID}' in parents and trashed=false",
        fields="files(id,name,mimeType)",
        pageSize=100,
    ).execute()

    dfs = []

    for f in results.get("files", []):
        if not f["name"].lower().endswith(".csv"):
            continue

        fh = io.BytesIO()

        if f["mimeType"].startswith("application/vnd.google-apps"):
            request = service.files().export_media(
                fileId=f["id"],
                mimeType="text/csv"
            )
        else:
            request = service.files().get_media(fileId=f["id"])

        downloader = MediaIoBaseDownload(fh, request)
        done = False
        while not done:
            _, done = downloader.next_chunk()

        fh.seek(0)
        dfs.append(pd.read_csv(fh))

    if not dfs:
        raise RuntimeError("âŒ CSV ë°ì´í„°ë¥¼ í•˜ë‚˜ë„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    return pd.concat(dfs, ignore_index=True)


# ğŸ‘‡ [ìˆ˜ì •ë¨] ì¤‘ë³µì„ ì œê±°í•˜ê³  ì•ˆì „ì¥ì¹˜(.strip)ë¥¼ ì¶”ê°€í•œ ìœ ì¼í•œ í•¨ìˆ˜
def send_to_notion(summary):
    url = "https://api.notion.com/v1/pages"
    
    # ì—¬ê¸°ì„œ NOTION_KEYê°€ ì—†ìœ¼ë©´ ì—ëŸ¬ë¥¼ ë„ì›Œì„œ ë°”ë¡œ ì•Œ ìˆ˜ ìˆê²Œ í•¨
    if not NOTION_KEY:
        print("âŒ ì˜¤ë¥˜: NOTION_KEY í™˜ê²½ë³€ìˆ˜ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return

    headers = {
        "Authorization": f"Bearer {NOTION_KEY.strip()}",  # ğŸ‘ˆ í•µì‹¬ ìˆ˜ì •: ê³µë°± ì œê±°!
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28",
    }

    payload = {
        "parent": {"database_id": NOTION_DB_ID},
        "properties": {
            "ë‚ ì§œ": {"title": [{"text": {"content": summary["date"]}}]},
            "ì²´ë¥˜ ì¥ì†Œ": {
                "multi_select": [{"name": p} for p in summary["places"]]
            },
            "ì´ë™ ìš”ì•½": {
                "rich_text": [{"text": {"content": summary["route"]}}]
            },
            "ì´ ì´ë™ê±°ë¦¬(km)": {"number": round(summary["distance_km"], 2)},
            "ì´ ì´ë™ì‹œê°„(ë¶„)": {"number": summary["duration_min"]},
            "ì§€ë„ ë§í¬": {"url": summary["map_url"]},
        },
    }

    r = requests.post(url, headers=headers, json=payload)
    if r.status_code == 200:
        print(f"âœ… {summary['date']} ë“±ë¡ ì™„ë£Œ")
    else:
        print("âŒ ë…¸ì…˜ ì˜¤ë¥˜:", r.text)


def main():
    try:
        df = download_all_csv()
    except Exception as e:
        print(f"âš ï¸ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    df.columns = df.columns.str.lower()
    df["datetime"] = pd.to_datetime(df["time"])

    if "accuracy" in df.columns:
        df = df[df["accuracy"] <= ACCURACY_LIMIT]

    now = datetime.now()
    today = now.date()

    if today <= datetime(2026, 1, 10).date():
        target_df = df[df["datetime"].dt.date <= today]
        print(f"ğŸ§± ì´ˆê¸° ëˆ„ì  ëª¨ë“œ (~ {today})")
    else:
        target_df = df[df["datetime"].dt.date == today]
        print(f"ğŸ“† ì¼ì¼ ëª¨ë“œ ({today})")

    target_df["date"] = target_df["datetime"].dt.date

    for date, g in target_df.groupby("date"):
        g = g.sort_values("datetime")
        if len(g) < 2:
            continue

        dist = 0
        path = [g.iloc[0]]

        for i in range(1, len(g)):
            d = haversine(
                g.iloc[i - 1].lat,
                g.iloc[i - 1].lon,
                g.iloc[i].lat,
                g.iloc[i].lon,
            )
            if d >= MIN_MOVE_DISTANCE:
                dist += d
                path.append(g.iloc[i])

        if len(path) < 2:
            continue

        duration = int(
            (g.iloc[-1].datetime - g.iloc[0].datetime).total_seconds() / 60
        )

        coords = "/".join([f"{p.lat},{p.lon}" for p in path])
        # Google Maps URL ìˆ˜ì • (ê²½ë¡œ ì‹œê°í™”ê°€ ë” ì˜ ë˜ë„ë¡)
        map_url = f"https://www.google.com/maps/dir/{coords}"

        summary = {
            "date": str(date),
            "places": ["ì´ë™"],
            "route": " â†’ ".join(
                [f"{p.lat:.3f},{p.lon:.3f}" for p in path[:5]]
            ),
            "distance_km": dist / 1000,
            "duration_min": duration,
            "map_url": map_url,
        }

        send_to_notion(summary)


if __name__ == "__main__":
    main()
