import os
import json
import pandas as pd
import requests
import math
import io
import time
from datetime import datetime, timedelta, date
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
from dateutil import parser

# ================= [ë¹„ë°€ë²ˆí˜¸ ë¡œë“œ] =================
try:
    from secrets import MY_KAKAO_KEY, MY_FOLDER_ID, MY_NOTION_KEY, MY_NOTION_DB_ID
    GDRIVE_SA_KEY = None 
    print("ğŸ’» ë¡œì»¬ PC ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
except ImportError:
    MY_KAKAO_KEY = os.environ.get("KAKAO_API_KEY")
    MY_FOLDER_ID = os.environ.get("GDRIVE_FOLDER_ID")
    MY_NOTION_KEY = os.environ.get("NOTION_KEY")
    MY_NOTION_DB_ID = os.environ.get("NOTION_DB_ID")
    GDRIVE_SA_KEY = os.environ.get("GDRIVE_SA_KEY")
    print("â˜ï¸ ì„œë²„(Github) ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")

# ================= [í•µì‹¬ ì„¤ì •ê°’] =================
AUTO_SWITCH_DATE = date(2026, 1, 10) # 1ì›” 10ì¼ë¶€í„° ìµœì‹  íŒŒì¼ 1ê°œë§Œ ì²˜ë¦¬

# [ì‚¬ìš©ì ìš”ì²­ ë°˜ì˜]
STAY_RADIUS = 70       # ë°˜ê²½ 50m
MIN_STAY_MINUTES = 10   # 5ë¶„ ì´ìƒ ì²´ë¥˜ ì‹œ ê¸°ë¡
MERGE_TIME_GAP_MINUTES = 30 

IS_CSV_UTC = False  
SMOOTHING_WINDOW = 3
ACCURACY_LIMIT = 50

MY_TAG_RULES = {
    "ë§ˆíŠ¸": "ğŸ›’ Market", "í¸ì˜ì ": "ğŸ›’ Market", "í•™êµ": "ğŸ« School", "ì´ˆë“±": "ğŸ« School",
    "ì—­": "ğŸš† Station", "ì¹´í˜": "â˜• Cafe", "ì»¤í”¼": "â˜• Cafe", "ë‹¤ì´ì†Œ": "ğŸ›ï¸ Shopping",
    "ì§‘": "ğŸ  Home", "ì•„íŒŒíŠ¸": "ğŸ  Home"
}

# ================= [ê¸°ëŠ¥ í•¨ìˆ˜] =================
def get_credentials():
    if os.path.exists('service_account.json'):
        return service_account.Credentials.from_service_account_file('service_account.json', scopes=['https://www.googleapis.com/auth/drive.readonly'])
    elif GDRIVE_SA_KEY:
        info = json.loads(GDRIVE_SA_KEY)
        return service_account.Credentials.from_service_account_info(info, scopes=['https://www.googleapis.com/auth/drive.readonly'])
    return None

def format_duration(minutes):
    minutes = int(minutes)
    hours = minutes // 60
    mins = minutes % 60
    return f"{hours}ì‹œê°„ {mins}ë¶„" if hours > 0 else f"{mins}ë¶„"

def haversine(lat1, lon1, lat2, lon2):
    R = 6371000
    phi1, phi2 = math.radians(lat1), math.radians(lat2)
    dphi = math.radians(lat2 - lat1)
    dlambda = math.radians(lon2 - lon1)
    a = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2) * math.sin(dlambda/2)**2
    return 2 * R * math.atan2(math.sqrt(a), math.sqrt(1-a))

def sync_fix_and_learn():
    print("ğŸ”„ ë…¸ì…˜ ë°ì´í„° ë™ê¸°í™” ë° ì¤‘ë³µ ê²€ì‚¬ ì¤€ë¹„ ì¤‘...")
    url = f"https://api.notion.com/v1/databases/{MY_NOTION_DB_ID}/query"
    headers = {"Authorization": f"Bearer {MY_NOTION_KEY}", "Content-Type": "application/json", "Notion-Version": "2022-06-28"}
    
    payload = {"page_size": 300, "sorts": [{"property": "ë°©ë¬¸ì¼ì‹œ", "direction": "descending"}]}
    
    existing_timestamps = set() 
    name_tag_memory = {} 
    
    try:
        resp = requests.post(url, headers=headers, json=payload)
        if resp.status_code == 200:
            results = resp.json().get("results", [])
            for page in results:
                props = page.get("properties", {})
                date_prop = props.get("ë°©ë¬¸ì¼ì‹œ", {}).get("date", {})
                if date_prop and date_prop.get("start"):
                    dt = parser.parse(date_prop["start"]).replace(tzinfo=None, second=0, microsecond=0)
                    existing_timestamps.add(dt)
                
                title_prop = props.get("ì´ë¦„", {}).get("title", [])
                p_name = title_prop[0].get("text", {}).get("content", "") if title_prop else ""
                tag_prop = props.get("íƒœê·¸", {}).get("multi_select", [])
                if p_name and tag_prop:
                    name_tag_memory[p_name] = tag_prop[0]['name']
            print(f"ğŸ“Š ê¸°ì¡´ ê¸°ë¡ {len(existing_timestamps)}ê°œ ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        print(f"âš ï¸ ë…¸ì…˜ ì½ê¸° ì—ëŸ¬: {e}")
    return existing_timestamps, name_tag_memory

def get_geo_info(lat, lng):
    headers = {"Authorization": f"KakaoAK {MY_KAKAO_KEY}"}
    url_addr = "https://dapi.kakao.com/v2/local/geo/coord2address.json"
    address_str = "ì£¼ì†Œ ë¯¸í™•ì¸"; place_name = ""
    try:
        resp = requests.get(url_addr, headers=headers, params={"x": lng, "y": lat}, timeout=3)
        if resp.status_code == 200:
            data = resp.json()
            if data['meta']['total_count'] > 0:
                doc = data['documents'][0]
                if doc['road_address']:
                    address_str = doc['road_address']['address_name']
                    if doc['road_address']['building_name']: place_name = doc['road_address']['building_name']
                else: address_str = doc['address']['address_name']
    except: pass

    if address_str != "ì£¼ì†Œ ë¯¸í™•ì¸":
        url_kwd = "https://dapi.kakao.com/v2/local/search/keyword.json"
        try:
            params = {"query": address_str, "x": lng, "y": lat, "radius": 50, "sort": "distance"}
            resp = requests.get(url_kwd, headers=headers, params=params, timeout=3)
            if resp.status_code == 200:
                data = resp.json()
                if data['meta']['total_count'] > 0: place_name = data['documents'][0]['place_name']
        except: pass
    if not place_name: place_name = address_str
    return place_name, address_str

def send_to_notion(visit_data, existing_timestamps, name_tag_memory):
    check_dt = visit_data['start'].replace(tzinfo=None, second=0, microsecond=0)
    if check_dt in existing_timestamps:
        print(f"ğŸ›¡ï¸ [ì¤‘ë³µ ì°¨ë‹¨] {visit_data['place_name']} ({check_dt.strftime('%m/%d %H:%M')})")
        return

    final_tag = "ğŸ“ ê¸°íƒ€"
    if visit_data['place_name'] in name_tag_memory:
        final_tag = name_tag_memory[visit_data['place_name']]
    else:
        for k, t in MY_TAG_RULES.items():
            if k in visit_data['place_name']: final_tag = t; break

    url = "https://api.notion.com/v1/pages"
    headers = {"Authorization": f"Bearer {MY_NOTION_KEY}", "Content-Type": "application/json", "Notion-Version": "2022-06-28"}
    
    payload = {
        "parent": {"database_id": MY_NOTION_DB_ID},
        "properties": {
            "ì´ë¦„": {"title": [{"text": {"content": visit_data['place_name']}}]},
            "ì£¼ì†Œ": {"rich_text": [{"text": {"content": visit_data['address']}}]},
            "íƒœê·¸": {"multi_select": [{"name": final_tag}]},
            "ì²´ë¥˜ì‹œê°„": {"rich_text": [{"text": {"content": format_duration(visit_data['duration'])}}]},
            "ë°©ë¬¸ì¼ì‹œ": {"date": {"start": visit_data['start'].isoformat(), "end": visit_data['end'].isoformat()}},
            "Lat": {"number": visit_data['lat']},
            "Lon": {"number": visit_data['lon']}
        }
    }

    try:
        resp = requests.post(url, headers=headers, json=payload)
        if resp.status_code == 200:
            print(f"âœ… ë“±ë¡: {visit_data['place_name']} ({check_dt.strftime('%H:%M')})")
            existing_timestamps.add(check_dt)
        else: print(f"âŒ ì‹¤íŒ¨: {resp.text}")
    except Exception as e: print(f"âŒ ì—ëŸ¬: {e}")

# [í•µì‹¬ ìˆ˜ì •] 403 ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ êµ¬ê¸€ ì‹œíŠ¸ í˜•ì‹ ëŒ€ì‘ ë¡œì§ ì¶”ê°€
def download_target_files():
    creds = get_credentials()
    if not creds: return []
    service = build('drive', 'v3', credentials=creds)
    
    today = datetime.now().date()
    if today < AUTO_SWITCH_DATE:
        query_params = {'orderBy': 'createdTime asc', 'pageSize': 100}
        print("ğŸ“‚ [ì „ì²´ ëª¨ë“œ] ëª¨ë“  ê³¼ê±° íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    else:
        query_params = {'orderBy': 'createdTime desc', 'pageSize': 1}
        print("ğŸ“‚ [ìµœì‹  ëª¨ë“œ] ìµœì‹  íŒŒì¼ 1ê°œë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    results = service.files().list(
        q=f"'{MY_FOLDER_ID}' in parents and trashed=false",
        fields="files(id, name, mimeType, createdTime)",
        **query_params
    ).execute()
    
    items = results.get('files', [])
    downloaded_files = []
    
    for item in items:
        # CSV í™•ì¥ìì´ê±°ë‚˜ êµ¬ê¸€ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ íƒ€ì…ì¸ ê²½ìš°ë§Œ ë‹¤ìš´ë¡œë“œ ì‹œë„
        if not (item['name'].lower().endswith('.csv') or item['mimeType'] == 'application/vnd.google-apps.spreadsheet'):
            continue
            
        print(f"   â¬‡ï¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {item['name']}")
        fh = io.BytesIO()
        try:
            # êµ¬ê¸€ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ í˜•ì‹ì¼ ê²½ìš° export_media ì‚¬ìš© (403 ì—ëŸ¬ ë°©ì§€)
            if item['mimeType'] == 'application/vnd.google-apps.spreadsheet':
                request = service.files().export_media(fileId=item['id'], mimeType='text/csv')
            else:
                request = service.files().get_media(fileId=item['id'])
                
            downloader = MediaIoBaseDownload(fh, request)
            done = False
            while done is False:
                status, done = downloader.next_chunk()
            
            fh.seek(0)
            df = pd.read_csv(fh, encoding='utf-8-sig')
            downloaded_files.append((df, item['name']))
        except Exception as e:
            print(f"   âŒ {item['name']} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            
    return downloaded_files

def process_clustering(df):
    points = df.to_dict('records')
    if not points: return []
    raw_visits = []; anchor = points[0]; cluster = [anchor]
    for i in range(1, len(points)):
        curr = points[i]
        dist = haversine(anchor['smooth_lat'], anchor['smooth_lon'], curr['smooth_lat'], curr['smooth_lon'])
        if dist < STAY_RADIUS: cluster.append(curr)
        else:
            start_t = cluster[0]['datetime']; end_t = cluster[-1]['datetime']
            duration = (end_t - start_t).total_seconds() / 60
            if duration >= MIN_STAY_MINUTES:
                avg_lat = sum(p['smooth_lat'] for p in cluster) / len(cluster)
                avg_lon = sum(p['smooth_lon'] for p in cluster) / len(cluster)
                api_name, api_addr = get_geo_info(avg_lat, avg_lon)
                raw_visits.append({'place_name': api_name, 'address': api_addr, 'lat': avg_lat, 'lon': avg_lon, 'start': start_t, 'end': end_t, 'duration': duration})
            anchor = curr; cluster = [curr]
    if cluster:
        start_t = cluster[0]['datetime']; end_t = cluster[-1]['datetime']
        duration = (end_t - start_t).total_seconds() / 60
        if duration >= MIN_STAY_MINUTES:
            avg_lat = sum(p['smooth_lat'] for p in cluster) / len(cluster); avg_lon = sum(p['smooth_lon'] for p in cluster) / len(cluster)
            api_name, api_addr = get_geo_info(avg_lat, avg_lon)
            raw_visits.append({'place_name': api_name, 'address': api_addr, 'lat': avg_lat, 'lon': avg_lon, 'start': start_t, 'end': end_t, 'duration': duration})
    return raw_visits

def merge_consecutive_visits(visits):
    if not visits: return []
    merged = [visits[0]]
    for current in visits[1:]:
        last = merged[-1]
        is_same_place = (current['place_name'] == last['place_name']) or (current['address'].replace(" ", "") == last['address'].replace(" ", ""))
        time_gap = (current['start'] - last['end']).total_seconds() / 60
        if is_same_place and time_gap <= MERGE_TIME_GAP_MINUTES:
            last['end'] = current['end']; last['duration'] = (last['end'] - last['start']).total_seconds() / 60
        else: merged.append(current)
    return merged

def main():
    print(f"ğŸš€ GPS ë¶„ì„ê¸° v2.3 (ë°˜ê²½:{STAY_RADIUS}m, ìµœì†Œì²´ë¥˜:{MIN_STAY_MINUTES}ë¶„)")
    existing_timestamps, name_tag_memory = sync_fix_and_learn()
    file_list = download_target_files()
    if not file_list: return

    for df, filename in file_list:
        print(f"\nğŸ“„ íŒŒì¼ ë¶„ì„ ì‹œì‘: {filename}")
        df.columns = df.columns.str.strip().str.lower()
        if 'time' not in df.columns and 'date' in df.columns: df['time'] = df['date'] + ' ' + df['time']
        df['datetime'] = pd.to_datetime(df['time'])
        if IS_CSV_UTC: df['datetime'] = df['datetime'] + timedelta(hours=9)
        df = df.sort_values('datetime')
        if 'accuracy' in df.columns: df = df[df['accuracy'] <= ACCURACY_LIMIT]
        if len(df) == 0: continue
        df['smooth_lat'] = df['lat'].rolling(window=SMOOTHING_WINDOW, min_periods=1, center=True).mean()
        df['smooth_lon'] = df['lon'].rolling(window=SMOOTHING_WINDOW, min_periods=1, center=True).mean()
        
        final_visits = merge_consecutive_visits(process_clustering(df))
        for visit in final_visits:
            send_to_notion(visit, existing_timestamps, name_tag_memory)

    print(f"\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
